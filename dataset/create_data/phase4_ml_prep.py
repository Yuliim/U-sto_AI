import pandas as pd
import numpy as np
import os
from datetime import datetime

# ---------------------------------------------------------
# ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
# ---------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOAD_DIR = os.path.join(BASE_DIR, "data_lifecycle")
SAVE_DIR = os.path.join(BASE_DIR, "data_ml")
os.makedirs(SAVE_DIR, exist_ok=True)

print("ğŸ“‚ [Phase 4] ë°ì´í„° ë¡œë“œ ì¤‘...")

# 1. ìš´ìš© ë§ˆìŠ¤í„° (ì „ì²´ ë¬¼í’ˆ ëª©ë¡)
df_op = pd.read_csv(os.path.join(LOAD_DIR, '04_01_operation_master.csv'))

# 2. ë¶ˆìš©/ì²˜ë¶„ ëª©ë¡
df_du = pd.read_csv(os.path.join(LOAD_DIR, '05_01_disuse_list.csv'))

# ---------------------------------------------------------
# 1. ë°ì´í„° ë³‘í•© ë° ì •ì œ (Data Cleaning)
# ---------------------------------------------------------
print("ğŸ§¹ 1. ë°ì´í„° ì •ì œ ë° ë³‘í•© ìˆ˜í–‰ ì¤‘...")

# (1) ìš´ìš©ì •ë³´ + ë¶ˆìš©ì •ë³´ ë³‘í•© (Left Join)
# ë³‘í•© ì „ì— 'ì‚¬ìœ ' ì»¬ëŸ¼ëª…ì„ ëª…í™•í•˜ê²Œ 'ë¶ˆìš©ì‚¬ìœ 'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
df_du_subset = df_du[['ë¬¼í’ˆê³ ìœ ë²ˆí˜¸', 'ë¶ˆìš©ì¼ì', 'ì‚¬ìœ ']].rename(columns={'ì‚¬ìœ ': 'ë¶ˆìš©ì‚¬ìœ '})

df_merged = pd.merge(
    df_op, 
    df_du_subset,
    on='ë¬¼í’ˆê³ ìœ ë²ˆí˜¸', 
    how='left'
)

# (2) í˜•ë³€í™˜ (String -> Datetime)
date_cols = ['ì·¨ë“ì¼ì', 'ì •ë¦¬ì¼ì', 'ë¶ˆìš©ì¼ì']
for col in date_cols:
    df_merged[col] = pd.to_datetime(df_merged[col], errors='coerce')

# (3) ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# 'ì •ë¦¬ì¼ì'ê°€ ë¹„ì–´ìˆìœ¼ë©´ 'ì·¨ë“ì¼ì'ë¡œ ëŒ€ì²´ (ìš´ìš© ì‹œì‘ ì‹œì )
df_merged['ì •ë¦¬ì¼ì'] = df_merged['ì •ë¦¬ì¼ì'].fillna(df_merged['ì·¨ë“ì¼ì'])

# (4) ì´ìƒì¹˜ ì²˜ë¦¬ (ì˜ˆì‹œ: ì·¨ë“ê¸ˆì•¡ì´ 0ì› ì´í•˜ì¸ ê²½ìš° ì œì™¸)
df_merged = df_merged[df_merged['ì·¨ë“ê¸ˆì•¡'] > 0].copy()

# ---------------------------------------------------------
# 2. íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering)
# ---------------------------------------------------------
print("âœ¨ 2. íŒŒìƒë³€ìˆ˜ ìƒì„± (Feature Engineering) ì¤‘...")

# ê¸°ì¤€ì¼ì (í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ìƒì˜ ì˜¤ëŠ˜)
current_date = pd.Timestamp(datetime.now().date())

# [Feature 1] ì´ ì‚¬ìš© ê¸°ê°„
df_merged['ê´€ì¸¡ì¢…ë£Œì¼ì'] = df_merged['ë¶ˆìš©ì¼ì'].fillna(current_date)
df_merged['ì´ì‚¬ìš©ì¼ìˆ˜'] = (df_merged['ê´€ì¸¡ì¢…ë£Œì¼ì'] - df_merged['ì·¨ë“ì¼ì']).dt.days

# [Feature 2] ì”ì—¬ë‚´êµ¬ì—°í•œ (RUL)
df_merged['ë²•ì ë‚´ìš©ì—°ìˆ˜'] = df_merged['ë‚´ìš©ì—°ìˆ˜'] * 365
df_merged['ì”ì—¬ë‚´ìš©ì—°ìˆ˜'] = df_merged['ë²•ì ë‚´ìš©ì—°ìˆ˜'] - df_merged['ì´ì‚¬ìš©ì¼ìˆ˜']

# [Feature 3] ì‚¬ìš© ê°•ë„ ì§€í‘œ (Usage Intensity)
def calculate_intensity(remark):
    if pd.isna(remark): return 1
    remark = str(remark)
    if any(x in remark for x in ['ì‹¤ìŠµ', 'ê³µìš©', 'ì„œë²„', 'ë„¤íŠ¸ì›Œí¬']):
        return 3 # ê°€í˜¹ ì¡°ê±´
    elif any(x in remark for x in ['ì—°êµ¬', 'ì—…ë¬´', 'ë””ìì¸']):
        return 2 # ì¼ë°˜ ì¡°ê±´
    else:
        return 1 # ë‹¨ìˆœ ë³´ê´€/ê¸°íƒ€

df_merged['ì‚¬ìš©ê°•ë„'] = df_merged['ë¹„ê³ '].apply(calculate_intensity)

# [Feature 4] ê³ ì¥ ë°œìƒ í”Œë˜ê·¸ (Failure Flag)
# ğŸ’¡ [ìˆ˜ì • í¬ì¸íŠ¸] ìœ„ì—ì„œ ë³€ê²½í•œ 'ë¶ˆìš©ì‚¬ìœ ' ì»¬ëŸ¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
df_merged['ê³ ì¥ë°œìƒì—¬ë¶€'] = df_merged['ë¶ˆìš©ì‚¬ìœ '].apply(lambda x: 1 if x == 'ê³ ì¥/íŒŒì†' else 0)

# [Feature 5] ê°€ê²©ëŒ€ë³„ ê°€ì¤‘ì¹˜ (log scale)
df_merged['ì·¨ë“ê¸ˆì•¡_Log'] = np.log1p(df_merged['ì·¨ë“ê¸ˆì•¡'])

# ---------------------------------------------------------
# 3. ë°ì´í„° ë¶„í•  (Train / Valid / Test)
# ---------------------------------------------------------
print("âœ‚ï¸ 3. ì‹œê³„ì—´ ê¸°ì¤€ ë°ì´í„° ë¶„í•  (7:2:1)...")

# (1) ì‹œê°„ ìˆœ ì •ë ¬
df_sorted = df_merged.sort_values(by='ì·¨ë“ì¼ì').reset_index(drop=True)

# (2) ì¸ë±ìŠ¤ ê³„ì‚°
n_total = len(df_sorted)
n_train = int(n_total * 0.7)
n_valid = int(n_total * 0.2)

# (3) ë°ì´í„° ìë¥´ê¸°
train_set = df_sorted.iloc[:n_train]
valid_set = df_sorted.iloc[n_train : n_train + n_valid]
test_set  = df_sorted.iloc[n_train + n_valid :]

print(f"   - ì „ì²´ ë°ì´í„°: {n_total}ê±´")
print(f"   - Train Set : {len(train_set)}ê±´")
print(f"   - Valid Set : {len(valid_set)}ê±´")
print(f"   - Test Set  : {len(test_set)}ê±´")

# ---------------------------------------------------------
# 4. ê²°ê³¼ ì €ì¥
# ---------------------------------------------------------
model_cols = [
    'ë¬¼í’ˆê³ ìœ ë²ˆí˜¸', 'G2B_ëª©ë¡ëª…', 'ë¬¼í’ˆë¶„ë¥˜ëª…',
    'ì·¨ë“ê¸ˆì•¡', 'ì·¨ë“ê¸ˆì•¡_Log', 'ë‚´ìš©ì—°ìˆ˜', 'ì‚¬ìš©ê°•ë„', 
    'ì·¨ë“ì¼ì', 'ë¶ˆìš©ì¼ì', 'ì´ì‚¬ìš©ì¼ìˆ˜', 'ì”ì—¬ë‚´ìš©ì—°ìˆ˜', 'ê³ ì¥ë°œìƒì—¬ë¶€',
    'ìš´ìš©ë¶€ì„œ'
]
available_cols = [c for c in model_cols if c in df_sorted.columns]

train_set[available_cols].to_csv(os.path.join(SAVE_DIR, 'train.csv'), index=False, encoding='utf-8-sig')
valid_set[available_cols].to_csv(os.path.join(SAVE_DIR, 'valid.csv'), index=False, encoding='utf-8-sig')
test_set[available_cols].to_csv(os.path.join(SAVE_DIR, 'test.csv'), index=False, encoding='utf-8-sig')

print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! 'data_ml' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")