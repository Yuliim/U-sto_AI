import os
import sys
import io
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.messages import HumanMessage, SystemMessage
from dotenv import load_dotenv
import os
# =====
# ì—­í• 
# - ìœ ì € ì…ë ¥
# - retriever í˜¸ì¶œ
# - LLM ì‘ë‹µ ì¶œë ¥
# ==========================================
# ğŸ”‡ [í™”ë©´ ì¶œë ¥ ì¸ì½”ë”© ì„¤ì •]
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')
# ==========================================

# # ğŸš¨ [í•„ìˆ˜] API í‚¤ ì…ë ¥ (ë³¸ì¸ í‚¤ë¡œ ë³€ê²½!) -> í‚¤ ê°€ì ¸ì˜¤ê¸°ë¡œ ë°”ê¿¨ìŠµë‹ˆë‹¤.
# os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
load_dotenv()  # .env íŒŒì¼ì„ os.environì— ë¡œë“œ (ì •ì„ ë°©ë²•)
# ë²¡í„° DB ê²½ë¡œ (ë°©ê¸ˆ ë§Œë“  í´ë” ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
DB_PATH = 'chroma_db'

def run_chat():
    print("ğŸ¤– ì±—ë´‡ ì‹œìŠ¤í…œ ë¡œë”© ì¤‘... (DB ì—°ê²°)")

    # 1. DB ë¡œë“œ (LangChain ë°©ì‹)
    if not os.path.exists(DB_PATH):
        print(f"âŒ ì˜¤ë¥˜: '{DB_PATH}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. create_vector_db.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    try:
        # LangChainìœ¼ë¡œ ì €ì¥í•œ DB ë¶ˆëŸ¬ì˜¤ê¸°
        vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
        print("âœ… ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # 2. AI ëª¨ë¸ ì„¤ì •
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.1)

    print("="*50)
    print("ğŸ“ ëŒ€í•™ ë¬¼í’ˆ ê´€ë¦¬ AI ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("('ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ë©´ êº¼ì§‘ë‹ˆë‹¤)")
    print("="*50)

    # 3. ì±„íŒ… ë£¨í”„
    while True:
        user_input = input("\nğŸ™‹ ì§ˆë¬¸í•˜ì„¸ìš”: ")
        
        if user_input.strip() == "ì¢…ë£Œ":
            print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
            break
        
        if not user_input.strip():
            continue

        print("Thinking...", end="", flush=True)

        # === ê²€ìƒ‰ ë‹¨ê³„ ===
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ 3ê°œ ì°¾ê¸°
        docs = vectordb.similarity_search(user_input, k=3)
        
        if not docs:
            print("\râš ï¸ ê´€ë ¨ ì •ë³´ë¥¼ ë§¤ë‰´ì–¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # ê²€ìƒ‰ëœ ë‚´ìš©ì„ í•©ì¹˜ê¸°
        context_text = "\n\n".join([doc.page_content for doc in docs])
        
        # ì¶œì²˜ í™•ì¸ (ë©”íƒ€ë°ì´í„° í™œìš©)
        sources = set([doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ') for doc in docs])

        # === ë””ë²„ê¹… ì¶œë ¥ (ì‚¬ìš©ìê°€ ì¢‹ì•„í–ˆë˜ ê¸°ëŠ¥) ===
        print(f"\rğŸ” [ì°¸ê³  ìë£Œ] {', '.join(sources)} ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        # === í”„ë¡¬í”„íŠ¸ ì‘ì„± ===
        system_instruction = f"""
        ë„ˆëŠ” ëŒ€í•™ ë¬¼í’ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì˜ 'AI ì±—ë´‡'ì´ì•¼.
        ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.
        
        [ê·œì¹™]
        1. ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ [ì°¸ê³  ìë£Œ] ë‚´ìš©ì— ê¸°ë°˜í•´ì„œ ë‹µë³€í•´.
        2. [ì°¸ê³  ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©ì€ "ì£„ì†¡í•©ë‹ˆë‹¤, ë§¤ë‰´ì–¼ì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ì–´ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë§í•´.
        3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì„œ ë§í•´ì¤˜.
        
        [ì°¸ê³  ìë£Œ]
        {context_text}
        """

        # === AI ë‹µë³€ ìƒì„± ===
        response = llm.invoke([
            SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content=system_instruction + f"\n\nì§ˆë¬¸: {user_input}")
        ])

        print(f"ğŸ¤– AI ë‹µë³€: {response.content}\n")
        print("-" * 50)

if __name__ == "__main__":
    run_chat()