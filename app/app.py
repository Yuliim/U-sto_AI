# import streamlit as st
# import os
# from dotenv import load_dotenv
# from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_community.vectorstores import Chroma
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser


# # 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì •ì˜ë¶€í„° í™•ì‹¤íˆ!)
# current_dir = os.path.dirname(os.path.abspath(__file__))
# env_path = os.path.join(current_dir, '.env')

# # 2. .env íŒŒì¼ ë¡œë“œ ì‹œë„
# load_dotenv(env_path)

# # 3. í‚¤ ê°€ì ¸ì˜¤ê¸°
# api_key = os.getenv("OPENAI_API_KEY")

# # 4. ì•ˆì „ì¥ì¹˜ (NameError ë°©ì§€ ë° ì—ëŸ¬ ë©”ì‹œì§€ í†µí•©)
# if not api_key:
#     # st.set_page_config(page_title="ì„¤ì • ì˜¤ë¥˜", page_icon="ğŸš¨")
#     st.error("ğŸš¨ **API í‚¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!**")
#     st.info(f"ğŸ“ í™•ì¸ ì¤‘ì¸ ê²½ë¡œ: {env_path}")
#     st.warning("**.env íŒŒì¼ í™•ì¸ ë¦¬ìŠ¤íŠ¸:**\n"
#                "1. íŒŒì¼ ì´ë¦„ì´ ì •í™•íˆ `.env` ì¸ê°€ìš”? (`.env.txt` ì•„ë‹˜)\n"
#                "2. íŒŒì¼ ì•ˆì— `OPENAI_API_KEY=sk-...` ë¼ê³  ì ì—ˆë‚˜ìš”?\n"
#                "3. ë“±í˜¸(=) ì•ë’¤ì— ê³µë°±ì€ ì—†ë‚˜ìš”?")
#     st.stop() 

# # 5. ì„±ê³µ ì‹œì—ë§Œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
# os.environ["OPENAI_API_KEY"] = api_key

# ###

# # 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
# st.set_page_config(page_title="ëŒ€í•™ ë¬¼í’ˆ ê´€ë¦¬ AI", page_icon="ğŸ“")
# st.title("ğŸ“ ëŒ€í•™ ë¬¼í’ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ AI ì±—ë´‡")
# st.caption("ğŸš€ ì´ì œ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.")

# # 2. ì§„ì§œ ë°ì´í„°(DB) ë¡œë“œ í•¨ìˆ˜
# @st.cache_resource
# def get_qa_chain():
#     # âš ï¸ ì¤‘ìš”: DB ë§Œë“¤ ë•Œ ì¼ë˜ ëª¨ë¸ê³¼ ë˜‘ê°™ì€ ê±¸ ì¨ì•¼ í•©ë‹ˆë‹¤.
#     embedding = OpenAIEmbeddings(model="text-embedding-3-small")
    
#     # ë°©ê¸ˆ ë§Œë“  'chroma_db' í´ë”ì™€ ì—°ê²°!
#     persist_directory = 'chroma_db'
    
#     if not os.path.exists(persist_directory):
#         st.error("âŒ 'chroma_db' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. create_vector_db.pyë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆë‚˜ìš”?")
#         return None

#     # DB ë¡œë“œ
#     vector_db = Chroma(persist_directory=persist_directory, embedding_function=embedding)

#     retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
#     # ë˜‘ë˜‘í•œ GPT-4o ì—°ê²°
#     llm = ChatOpenAI(model_name="gpt-4o", temperature=0.1)

#     # í”„ë¡¬í”„íŠ¸(ì§€ì‹œì‚¬í•­) ì„¤ì •
#     template = """
#     ë‹¹ì‹ ì€ ëŒ€í•™ ë¬¼í’ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì˜ ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
#     ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
#     ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ "ì£„ì†¡í•©ë‹ˆë‹¤, ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”.

#     [ì°¸ê³  ìë£Œ]:
#     {context}

#     ì§ˆë¬¸: {question}
#     ë‹µë³€:
#     """
#     prompt = PromptTemplate.from_template(template)

#     chain = (
#         {
#             "context": retriever,
#             "question": lambda x: x
#         }
#         | prompt
#         | llm
#         | StrOutputParser()
#     )
#     return chain

# # 3. ì±„íŒ… UI êµ¬ì„±
# if "messages" not in st.session_state:
#     st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬¼í’ˆ ê´€ë¦¬ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# for msg in st.session_state.messages:
#     st.chat_message(msg["role"]).write(msg["content"])

# # 4. ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
# if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
#     # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
#     st.session_state.messages.append({"role": "user", "content": query})
#     st.chat_message("user").write(query)

#     # AI ë‹µë³€ ìƒì„±
#     with st.chat_message("assistant"):
#         chain = get_qa_chain()
#         if chain:
#             with st.spinner("ë§¤ë‰´ì–¼ì„ ì°¾ì•„ë³´ëŠ” ì¤‘ì…ë‹ˆë‹¤... ğŸ“š"):
#                 try:
#                     # ë‹µë³€ ìš”ì²­
#                     response_text = chain.invoke(query)
                    
#                     # # ì¶œì²˜ ì •ë¦¬ (ì¤‘ë³µ ì œê±°)
#                     # sources = set([doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ') for doc in source_docs])
                    
#                     # í™”ë©´ ì¶œë ¥
#                     st.write(response_text)
                    
#                     # # ì¶œì²˜ í‘œì‹œ (ì‘ê²Œ)
#                     # if sources:
#                     #     st.caption(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ: {', '.join(sources)}")
                    
#                     # ê¸°ë¡ ì €ì¥
#                     st.session_state.messages.append({"role": "assistant", "content": response_text})
                
#                 except Exception as e:
#                     st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")