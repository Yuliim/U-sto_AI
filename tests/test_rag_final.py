# ì¶”í›„ ì‚­ì œí•  íŒŒì¼ì…ë‹ˆë‹¤.
import os
import sys
import io
import json
import re
import difflib
from dotenv import load_dotenv

# [ìˆ˜ì •ëœ ë¶€ë¶„] ìµœì‹  LangChain ë²„ì „ í˜¸í™˜ ê²½ë¡œ
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage 

# [í™”ë©´ ì¶œë ¥ ì„¤ì •]
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')

load_dotenv()

# =========================================================
# [ì„¤ì •] ê²€ìƒ‰ + LLM ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸
# =========================================================
SEARCH_K = 50 
DATA_FILE = "dataset/qa_output/manual_qa_final.json"
CHROMA_DB_PATH = "./chroma_db"

def safe_import():
    try:
        from langchain_openai import OpenAIEmbeddings
        from langchain_chroma import Chroma
        from langchain_community.retrievers import BM25Retriever
        from langchain_core.documents import Document
        return OpenAIEmbeddings, Chroma, BM25Retriever, Document
    except ImportError as e:
        print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—ëŸ¬: {e}")
        sys.exit(1)

def extract_question_from_text(text):
    match = re.search(r"Q:\s*(.*?)(?:\n|$)", text)
    return match.group(1).strip() if match else ""

def calculate_fuzzy_score(query, target):
    if not target: return 0.0
    norm_query = re.sub(r"\s+", "", query)
    norm_target = re.sub(r"\s+", "", target)
    matcher = difflib.SequenceMatcher(None, norm_query, norm_target)
    return matcher.ratio()

def main():
    print("ğŸš€ [ìµœì¢…] ê²€ìƒ‰(Top 3) -> LLM ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸\n")
    
    # 1. ê²€ìƒ‰ ì¤€ë¹„
    OpenAIEmbeddings, Chroma, BM25Retriever, Document = safe_import()

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    docs = []
    for item in data:
        page_content = f"[{item.get('category')}] {item.get('title')}\nQ: {item.get('question')}\nA: {item.get('answer')}"
        metadata = {"source": item.get("source")}
        docs.append(Document(page_content=page_content, metadata=metadata))

    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = SEARCH_K
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": SEARCH_K})

    # 2. ì§ˆë¬¸
    user_query = "ë¬¼í’ˆ ë°˜ë‚© ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë¼?"
    print(f"ğŸ‘¤ ì‚¬ìš©ì ì§ˆë¬¸: '{user_query}'")
    print("-" * 50)

    # 3. ê²€ìƒ‰ ì‹¤í–‰ (Hybrid + Fuzzy)
    bm25_res = bm25_retriever.invoke(user_query)
    chroma_res = chroma_retriever.invoke(user_query)
    
    score_map = {}
    # BM25 ì ìˆ˜
    for i, doc in enumerate(bm25_res):
        key = doc.page_content
        if key not in score_map: score_map[key] = {'doc': doc, 'score': 0}
        score_map[key]['score'] += (1.0/(i+1))

    # Chroma ì ìˆ˜
    for i, doc in enumerate(chroma_res):
        key = doc.page_content
        if key not in score_map: score_map[key] = {'doc': doc, 'score': 0}
        score_map[key]['score'] += (1.0/(i+1))

    # Fuzzy Bonus
    for key, item in score_map.items():
        doc_q = extract_question_from_text(key)
        if calculate_fuzzy_score(user_query, doc_q) >= 0.4:
            item['score'] += 10.0

    sorted_items = sorted(score_map.values(), key=lambda x: x['score'], reverse=True)
    top_3_docs = [item['doc'] for item in sorted_items[:3]]

    # 4. LLMì—ê²Œ ë‹µë³€ ìš”ì²­
    print("ğŸ¤– LLMì´ ë‹µë³€ì„ ìƒê° ì¤‘ì…ë‹ˆë‹¤...\n")
    
    context_text = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{d.page_content}" for i, d in enumerate(top_3_docs)])
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0) 
    
    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ë‚´ ê·œì • ì±—ë´‡ì…ë‹ˆë‹¤. 
    ì•„ë˜ ì œê³µëœ [ê²€ìƒ‰ëœ ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    ê²€ìƒ‰ëœ ë¬¸ì„œì— ì •ë‹µì´ ìˆë‹¤ë©´, ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    """
    
    user_prompt = f"""
    [ê²€ìƒ‰ëœ ë¬¸ì„œ]
    {context_text}

    [ì§ˆë¬¸]
    {user_query}
    """

    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    print("ğŸ“¢ [AI ë‹µë³€ ê²°ê³¼]")
    print("=" * 60)
    print(response.content)
    print("=" * 60)
    
    # 5. ê²€ì¦
    if "ìš´ìš© ë¶€ì„œì—ì„œ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜" in response.content or "ê³ ì¥" in response.content:
        print("\nâœ… ì„±ê³µ! LLMì´ ì˜¬ë°”ë¥¸ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì •ë‹µì„ ë§í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë‹µë³€ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main()