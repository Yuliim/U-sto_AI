from langchain_openai import ChatOpenAI 
from langchain_core.messages import SystemMessage, HumanMessage  # 메시지 타입
from typing import Dict  # 타입 힌트
import json  # JSON 처리
import re    # 정규식 처리
import traceback

# MIN_TRUNCATION_RATIO: 텍스트를 자를 때, 마침표(.)를 찾더라도
# 전체 허용 길이의 최소 50% 이상은 유지하도록 보장하는 비율.
# (이유: 마침표가 문장 초반에 나와서 텍스트가 너무 많이 잘려나가는 것을 방지)
MIN_TRUNCATION_RATIO = 0.5

# 프롬프트 템플릿 설정 (유지보수를 위해 로직과 분리하여 최상단 배치)
# 주의: 아래 템플릿의 이중 중괄호 {{ ... }}는 오타가 아닙니다.
# Python의 .format() 메서드 사용 시, 변수가 아닌 '문자 그대로의 중괄호'를 표현하기 위해 이스케이프(Escape) 처리한 것입니다.
# 실제 LLM에게 프롬프트가 전달될 때는 홀수 중괄호 { ... } 로 정상 변환됩니다.
QA_PROMPT_TEMPLATE = """
아래 [내용]을 완벽하게 이해한 뒤, 사용자가 이 정보를 찾기 위해 물어볼 법한 질문(question)과 그에 대한 답변(answer)을 생성해.

[작성 규칙]
1. 질문은 "어떻게 해?", "뭐야?" 처럼 대화체로 작성하되, 핵심 키워드(예: 반납, 불용, 처분 등)를 반드시 포함할 것.
2. 답변은 매뉴얼 내용을 기반으로 상세하게 작성할 것.
3. 카테고리는 주제를 대표하는 단어 1개(규정, 절차, 시스템, 오류해결 등)로 추출할 것.

반드시 아래 JSON 형식으로만 출력해:
{{
  "question": "생성된 질문",
  "answer": "생성된 답변",
  "category": "추출된 카테고리"
}}

[내용]:
{context}
"""

def _extract_json(text: str) -> Dict:
    # LLM 응답에서 JSON만 추출하는 내부 함수
    try:
        return json.loads(text)  # 바로 파싱 시도
    except json.JSONDecodeError:
        # 마크다운 코드블록 제거
        text = text.replace("```json", "").replace("```", "")
        # 중괄호 {} 로 감싸진 부분만 정규식으로 추출
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            return json.loads(match.group())  # 추출된 JSON 파싱
    return {}  # 실패 시 빈 dict 반환

def convert_to_qa(item: Dict, llm: ChatOpenAI) -> Dict:
    """
    Convert a single ingested document item into a Q/A pair and attach metadata.
    This function sends the document content to an LLM with a prompt that asks it to
    generate a `question`, `answer`, and a single-word `category` that describes the
    topic of the content (e.g., "규정", "절차", "시스템", "일반"). The LLM output is
    expected to be valid JSON and is post-processed to ensure that required fields
    and metadata are present.
    Parameters
    ----------
    item : Dict
        A dictionary representing a single document chunk. The converter expects
        the following keys (all are optional but missing/invalid values may cause
        the function to return an empty dict):
        - ``"chapter"`` (str): Logical chapter or section name for the content.
        - ``"title"`` (str): Title or subtitle of the content chunk.
        - ``"content"`` (str): Main text body that will be used to generate the
          question/answer pair. If this value is shorter than 10 characters, the
          function treats it as noise and returns ``{}``.
        - ``"source"`` (str): Identifier of the origin file or document (e.g. set
          by the loader), which will be attached as metadata to the result.
    llm : ChatOpenAI
        A LangChain-compatible chat model instance used to invoke the LLM.
    Returns
    -------
    Dict
        On success, a dictionary with the following structure:
        - ``"question"`` (str): Question generated by the LLM from the content.
        - ``"answer"`` (str): Answer generated by the LLM that matches the question.
        - ``"category"`` (str): Single-word category extracted by the LLM describing
          the topic of the content. If the LLM does not return a ``"category"`` key,
          it is automatically set to ``"General"``.
        - ``"source"`` (str): Copied from ``item["source"]`` when available.
        - ``"title"`` (str): Copied from ``item["title"]`` (may be empty string).
        - ``"chapter"`` (str): Copied from ``item["chapter"]`` (may be empty string).
        If the input content is too short, if the LLM output cannot be parsed as
        JSON, if the parsed JSON does not contain both ``"question"`` and
        ``"answer"``, or if a runtime error occurs during LLM invocation, the
        function returns an empty dictionary ``{}``.
    Error handling
    --------------
    Any exception raised while invoking the LLM or parsing its response is caught.
    In such cases, a detailed error message and traceback are printed to standard
    output (including the title of the problematic document when available), and
    the function returns ``{}`` without raising the exception further.
    """

    # 입력 텍스트 구성 (챕터와 제목도 포함하여 AI에게 문맥 제공)
    chapter = item.get('chapter', '')
    title = item.get('title', '')
    content = item.get('content', '')

    # 문맥 정보를 합쳐서 하나의 텍스트로 만듦
    context_text = f"Chapter: {chapter}\nTitle: {title}\nContent: {content}"

    # 너무 짧은 데이터 필터링 (노이즈 제거)
    if len(content) < 10:
        return {}

    # 프롬프트 완성 (템플릿에 데이터 끼워넣기)
    # 2000자 제한을 두어 토큰 비용 절약
    MAX_CONTEXT_CHARS = 2000

    if len(context_text) > MAX_CONTEXT_CHARS:
        # 긴 컨텍스트는 잘라내되, 사용자가 알 수 있도록 로그를 남김
        print(
            f"[Info] Context text truncated from {len(context_text)} to "
            f"{MAX_CONTEXT_CHARS} characters for title='{title}'"
        )
        truncated_context = context_text[:MAX_CONTEXT_CHARS]
        # 가능한 경우 문장 단위(마침표 기준)로 끊어서 부자연스러운 잘림을 최소화
        last_sentence_end = truncated_context.rfind(".")

        # [수정됨] 매직 넘버(0.5)를 상수로 대체하여 가독성 개선
        # 마침표가 발견되었고, 그 위치가 최소 유지 비율(50%)보다 뒤에 있을 때만 자름
        if last_sentence_end != -1 and last_sentence_end > MAX_CONTEXT_CHARS * MIN_TRUNCATION_RATIO:
            truncated_context = truncated_context[: last_sentence_end + 1]
    else:
        truncated_context = context_text
    
    final_prompt = QA_PROMPT_TEMPLATE.format(context=truncated_context)

    # LLM 호출
    try:
        response = llm.invoke([
            SystemMessage(content="너는 데이터셋 생성을 돕는 AI야. 반드시 유효한 JSON만 출력해."),
            HumanMessage(content=final_prompt)
        ])
        
        qa = _extract_json(response.content)  # JSON 파싱
    
    except Exception as e:
        print(f"[Error] LLM 변환 중 치명적인 오류 발생")
        print(f" - 에러 메시지: {e}")
        print(f" - 문제 발생 문서: {item.get('title', '제목 없음')}") # 어떤 문서인지 알려줌
        print("Detailed Traceback:")
        print(traceback.format_exc())
        return {}

    # 결과 검증 및 메타데이터(Metadata) 부착
    # 원본 데이터의 정보를 결과물에 꼬리표로 붙여줍니다.
    if "question" in qa and "answer" in qa:
        qa["source"] = item.get("source")   # 파일명 (loader.py에서 가져옴)
        qa["title"] = title                 # 원본 소제목 (검색 시 활용)
        qa["chapter"] = chapter             # 챕터 정보 (필터링 시 활용)
        
        # category는 위에서 AI가 생성했으므로 그대로 둠 (혹은 강제로 지정 가능)
        if "category" not in qa:
            qa["category"] = "General"
            
        return qa

    return {}